{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHAcdmn0vXMi"
      },
      "source": [
        "# Importing Importanta packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t8OdRR199GH",
        "outputId": "cd63e939-d5a1-4310-8901-4ab9f1c95f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning-utilities\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (4.14.1)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n"
          ]
        }
      ],
      "source": [
        "# Install the torchmetrics package for storing loss, evaluation metrics, etc.\n",
        "!pip install lightning-utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x8DcQ8t-BYn"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GzalSjJ-CEu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models # Import models module\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.models import EfficientNet_B0_Weights\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchmetrics import MeanMetric, Accuracy\n",
        "from torchmetrics import ConfusionMatrix, Accuracy, Precision, Recall, F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBP-XcpY-D8m"
      },
      "outputs": [],
      "source": [
        "# Make sure to change runtime to GPU\n",
        "# Check if GPU is avaiable\n",
        "device = \"cuda\" if torch.cuda.is_available() \\\n",
        "          else \"mps\" if torch.mps.is_available() \\\n",
        "          else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2raflB3viLZ"
      },
      "source": [
        "# Getting The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX73FxZY-b2T"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"farjanakabirsamanta/skin-cancer-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nT2sGYoAI_F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List downloaded files\n",
        "print(\"Dataset contents:\", os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAp_2GPPAO7Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Checking for any subdirectories inside the downloaded dataset path\n",
        "print(\"Subdirectories:\", [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXF4uOqUASZq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Update path to the correct subdirectory\n",
        "dataset_path = os.path.join(path, \"Skin Cancer\")\n",
        "\n",
        "# List contents in the new path\n",
        "print(\"Dataset contents:\", os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19qaycwOAhJC"
      },
      "outputs": [],
      "source": [
        "import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WonuwckTAXyG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Look for images inside the \"Skin Cancer\" directory\n",
        "image_files = glob.glob(os.path.join(dataset_path, \"*.jpg\"))\n",
        "print(f\"Total images found: {len(image_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJW-VL7xAqWg"
      },
      "outputs": [],
      "source": [
        "#I can't find where the images of dataset so I will dig more to see if there is more subfolders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZImoB5GAi_8"
      },
      "outputs": [],
      "source": [
        "# List subdirectories within \"Skin Cancer\"\n",
        "print(\"Subdirectories inside 'Skin Cancer':\", [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-cSESm9A1cL"
      },
      "outputs": [],
      "source": [
        "# Update dataset path to point to the actual files\n",
        "dataset_path = os.path.join(path, \"Skin Cancer\", \"Skin Cancer\")\n",
        "\n",
        "# List the files inside the correct dataset folder\n",
        "print(\"Updated dataset contents:\", os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaxwzZBDA9Fk"
      },
      "outputs": [],
      "source": [
        "# Look for images inside the correct dataset folder\n",
        "image_files = glob.glob(os.path.join(dataset_path, \"*.jpg\"))\n",
        "print(f\"Total images found: {len(image_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p18i5-Xc-gpG"
      },
      "outputs": [],
      "source": [
        "# Load metadata CSV\n",
        "# The original path was incorrect. Adjust to find the correct location of the metadata file.\n",
        "# The metadata file might be in the base directory of the dataset, not in the 'Skin Cancer/Skin Cancer' subfolder.\n",
        "metadata_path = os.path.join(path, \"HAM10000_metadata.csv\")\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Attach correct file paths to images\n",
        "df['filepath'] = df['image_id'].apply(lambda x: os.path.join(dataset_path, f\"{x}.jpg\"))\n",
        "\n",
        "# Verify if all image files exist\n",
        "missing_files = df['filepath'].apply(lambda x: not os.path.exists(x)).sum()\n",
        "print(f\"Missing images: {missing_files}\")\n",
        "\n",
        "# Check dataset distribution\n",
        "print(df['dx'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMatz-ojBORM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQO87SZf-wW0"
      },
      "outputs": [],
      "source": [
        "# Get unique classes\n",
        "classes = df['dx'].unique()\n",
        "\n",
        "# Create a figure\n",
        "fig, axes = plt.subplots(2, 4, figsize=(15, 6))  # 2 rows, 4 columns\n",
        "\n",
        "for ax, class_name in zip(axes.flat, classes):\n",
        "    # Select a random image from this class\n",
        "    sample = df[df['dx'] == class_name].sample(1)\n",
        "    img_path = sample['filepath'].values[0]\n",
        "\n",
        "    # Load and display the image\n",
        "    img = Image.open(img_path)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(class_name)\n",
        "    ax.axis('off')  # Hide axes\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WhhwROE7Lu"
      },
      "outputs": [],
      "source": [
        "num_class = len(classes)\n",
        "print(\"Number of classes:\", num_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_gKQWbgvztX"
      },
      "source": [
        "# Applying Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tl1k-_j-721"
      },
      "outputs": [],
      "source": [
        "# data augmentation for train\n",
        "# --- build transforms using the weightsâ€™ recommended preprocessing ---\n",
        "weights = EfficientNet_B0_Weights.IMAGENET1K_V1  # NEW\n",
        "\n",
        "train_transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(224, scale=(0.85, 1.0)),  # CHANGED: EfficientNet-B0 is 224x224\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.1),\n",
        "    v2.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),\n",
        "    v2.ToImage(), # Convert to tensor before normalization\n",
        "    v2.ToDtype(torch.float32, scale=True), # Convert to float32 before normalization\n",
        "    v2.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # CHANGED\n",
        "])\n",
        "\n",
        "val_transforms = v2.Compose([\n",
        "    v2.Resize(int(224 * 1.15)),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.ToImage(), # Convert to tensor before normalization\n",
        "    v2.ToDtype(torch.float32, scale=True), # Convert to float32 before normalization\n",
        "    v2.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # CHANGED\n",
        "])\n",
        "\n",
        "test_transforms = val_transforms  # keep same as validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncWFR6e1-_jo"
      },
      "outputs": [],
      "source": [
        "# data augmentation for validation and test\n",
        "eval_transform = v2.Compose([\n",
        "    v2.Resize((224, 224)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=weights.transforms().mean,\n",
        "                 std=weights.transforms().std),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpVboDcVCeDR"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(dataset_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7uzYofOvsCo"
      },
      "source": [
        "# Organizing the folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgM1BJcODrAe"
      },
      "outputs": [],
      "source": [
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fHQEQaNDyAT"
      },
      "outputs": [],
      "source": [
        "#My Dataset was not organized by class so I have to reoorganize it to be able to use for train, test, and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ3u3kCfDn4x"
      },
      "outputs": [],
      "source": [
        "# Assuming df is your metadata DataFrame and dataset_path is the path to your images\n",
        "# Create a new directory to hold the organized dataset\n",
        "organized_dataset_path = os.path.join(\"/kaggle/working\", \"organized_skin_cancer_dataset\")\n",
        "os.makedirs(organized_dataset_path, exist_ok=True)\n",
        "\n",
        "# Get unique classes from your metadata\n",
        "classes = df['dx'].unique()\n",
        "\n",
        "# Create subfolders for each class in the new directory\n",
        "for class_name in classes:\n",
        "    class_dir = os.path.join(organized_dataset_path, class_name)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "# Move images to their respective class folders\n",
        "for index, row in df.iterrows():\n",
        "    img_path = row['filepath']  # Path to the image\n",
        "    class_name = row['dx']  # Class label\n",
        "    destination_path = os.path.join(organized_dataset_path, class_name, os.path.basename(img_path))\n",
        "    shutil.copy(img_path, destination_path)  # Copy the image to the new location\n",
        "\n",
        "# Now, update your dataset_path to the new organized directory\n",
        "dataset_path = organized_dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd9UHVIpEBMB"
      },
      "outputs": [],
      "source": [
        "dataset_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UXkHTHVv8oP"
      },
      "source": [
        "# Dividing The Train, Val, Test in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVVqz1Y2_Dyc"
      },
      "outputs": [],
      "source": [
        "# Read datasets\n",
        "train_data = torchvision.datasets.ImageFolder(dataset_path, transform=train_transforms)\n",
        "val_data = torchvision.datasets.ImageFolder(dataset_path, transform=eval_transform)\n",
        "test_set = torchvision.datasets.ImageFolder(dataset_path, transform=eval_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lWWCAzEFxl"
      },
      "outputs": [],
      "source": [
        "# Split the original train data into training (80%) and validation (20%)\n",
        "# Random split (only splitting indices, datasets are independent)\n",
        "train_size = int(0.8 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_indices, val_indices = torch.utils.data.random_split(range(len(train_data)),\n",
        "                                                          [train_size, val_size])\n",
        "# train_data and val_data have different transforms\n",
        "train_set = torch.utils.data.Subset(train_data, train_indices)\n",
        "val_set = torch.utils.data.Subset(val_data, val_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uRDwUn8EKkJ"
      },
      "outputs": [],
      "source": [
        "# Define the data loaders for the training, validation, and test sets\n",
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXlMQsLHEybz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIZolOgwGF9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2NqwfFNEOUN"
      },
      "outputs": [],
      "source": [
        "# get a pretrain EfficientNet-B0\n",
        "#model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "# --- EfficientNet-B0 backbone with ImageNet-1K weights ---\n",
        "\n",
        "# Load pretrained EfficientNet-B0\n",
        "model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# (Optional) adjust dropout (default is 0.2 at classifier[0])\n",
        "# model.classifier[0].p = 0.2  # leave as-is unless you want to tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeDIsqP8E-wy"
      },
      "outputs": [],
      "source": [
        "# Add a new layer/change the last layer\n",
        "#model.fc = nn.Linear(model.fc.in_features, num_class)\n",
        "# Replace classifier head (EfficientNet-B0 classifier is [Dropout, Linear])\n",
        "in_features = model.classifier[1].in_features  # 1280 for B0\n",
        "model.classifier[1] = nn.Linear(in_features, num_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4lZlVG3FRGR"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oI5rnvQFWa2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Defined Cross-Entropy Loss\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defined Optimizer (Adam with learning rate 1e-4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeduVZ_4wPN1"
      },
      "source": [
        "# Getting Read of Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ScVsUvbFdjf"
      },
      "outputs": [],
      "source": [
        "# a function for training one epoch\n",
        "def train_one_epoch(model, dataloader):\n",
        "  # Prepare for storing loss and accuracy\n",
        "  losses = MeanMetric().to(device)\n",
        "  acc = Accuracy(task='multiclass', num_classes=10).to(device)\n",
        "  model.train() # set model to train mode\n",
        "  # a loop to iterate input(X) and label(Y) for all mini-batches\n",
        "  for X, Y in tqdm(dataloader):\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "    optimizer.zero_grad() # reset optimizer\n",
        "    preds = model(X) # model forward\n",
        "    loss = criteria(preds, Y) # calculate loss\n",
        "    loss.backward() # compute gradients via backpropagation\n",
        "    optimizer.step() # perform gradient descent\n",
        "    preds = preds.argmax(dim=1) # obtain the final predicted class\n",
        "    losses.update(loss, X.size(0)) # store loss per batch\n",
        "    acc.update(preds, Y) # store accuracy per batch\n",
        "  return losses.compute().item(), acc.compute().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Dx7RcCF-uy"
      },
      "outputs": [],
      "source": [
        "# a function for validation one epoch\n",
        "def validation_one_epoch(model, dataloader):\n",
        "  # Prepare for storing loss and accuracy\n",
        "  losses = MeanMetric().to(device)\n",
        "  acc = Accuracy(task='multiclass', num_classes=10).to(device)\n",
        "  model.eval() # set model to validation mode\n",
        "  with torch.no_grad(): # disables gradient computation for evaluation\n",
        "    # a loop to iterate input(X) and label(Y) for all mini-batches\n",
        "    for X, Y in tqdm(dataloader):\n",
        "      X = X.to(device)\n",
        "      Y = Y.to(device)\n",
        "      preds = model(X) # model forward\n",
        "      loss = criteria(preds, Y) # calculate loss\n",
        "      preds = preds.argmax(dim=1) # obtain the final predicted class\n",
        "      losses.update(loss, X.size(0)) # store loss per batch\n",
        "      acc.update(preds, Y) # store accuracy per batch\n",
        "  return losses.compute().item(), acc.compute().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS8mvgShGBfk"
      },
      "outputs": [],
      "source": [
        "# Prepare for storing loss and accuracy\n",
        "best_val_loss = float('inf')  # Initialize best_val_loss to infinity\n",
        "history = pd.DataFrame() # store statics for each epoch\n",
        "epochs = 20 # number of epochs\n",
        "# a loop for epochs\n",
        "for i in range(0, epochs):\n",
        "  # train one epoch\n",
        "  train_loss, train_acc = train_one_epoch(model, train_dataloader)\n",
        "  # validation one epoch\n",
        "  val_loss, val_acc = validation_one_epoch(model, val_dataloader)\n",
        "  # store and print loss and accuracy per epoch\n",
        "  statistics = pd.DataFrame({\n",
        "      \"epoch\": [i],   \"train_loss\": [train_loss],\n",
        "                      \"train_acc\": [train_acc],\n",
        "                      \"val_loss\": [val_loss],\n",
        "                      \"val_acc\": [val_acc]})\n",
        "  history = pd.concat([history, statistics], ignore_index=True)\n",
        "  print(statistics.to_dict(orient=\"records\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, torch\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"   # force sync for clearer traceback\n",
        "\n",
        "# sanity check one batch from each loader\n",
        "# def sanity_check(loader, model):\n",
        "#     model.eval()\n",
        "#     xb, yb = next(iter(loader))\n",
        "#     print(\"xb dtype/shape:\", xb.dtype, xb.shape)\n",
        "#     print(\"yb dtype/shape:\", yb.dtype, yb.shape, \"min/max:\", int(yb.min()), int(yb.max()))\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(xb.to(next(model.parameters()).device))\n",
        "#     print(\"logits shape:\", logits.shape)\n",
        "#     num_classes = logits.shape[1]\n",
        "#     print(\"num_classes (from model):\", num_classes)\n",
        "#     assert yb.min().item() >= 0, \"Found negative labels\"\n",
        "#     assert yb.max().item() < num_classes, f\"Label {int(yb.max())} >= num_classes {num_classes}\"\n",
        "\n",
        "# sanity_check(train_dataloader, model)\n",
        "# sanity_check(val_dataloader, model)"
      ],
      "metadata": {
        "id": "KuF6dHsbWldg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ18QdsLwVf6"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUESMP7JwYQg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMMqYY_qGthU"
      },
      "outputs": [],
      "source": [
        "# Create a figure with two subplots (side by side)\n",
        "plt.figure(figsize=(8, 3))\n",
        "# Plot Loss Curve (Train + Validation)\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
        "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\", color=\"blue\")\n",
        "plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation\", color=\"red\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Plot Accuracy Curve (Train + Validation)\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
        "plt.plot(history[\"epoch\"], history[\"train_acc\"], label=\"Train\", color=\"blue\")\n",
        "plt.plot(history[\"epoch\"], history[\"val_acc\"], label=\"Validation\", color=\"red\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Adjust layout and show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keasoaQdGy5L"
      },
      "outputs": [],
      "source": [
        "# prepare for storing evaluation metrics\n",
        "test_acc = Accuracy(task='multiclass', num_classes=num_class).to(device)\n",
        "test_confusion_matrix=ConfusionMatrix(task=\"multiclass\", num_classes=num_class).to(device)\n",
        "test_precision = Precision(task=\"multiclass\", num_classes=num_class, average=\"macro\").to(device)\n",
        "test_recall = Recall(task=\"multiclass\", num_classes=num_class, average=\"macro\").to(device)\n",
        "test_f1_score = F1Score(task=\"multiclass\", num_classes=num_class, average=\"macro\").to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "  for X, Y in test_dataloader:\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "    preds = model(X) # model forward\n",
        "    preds = preds.argmax(dim=1) # obtain the final predicted class\n",
        "    # store loss and accuracy per batc\n",
        "    test_confusion_matrix.update(preds, Y)\n",
        "    test_acc.update(preds, Y)\n",
        "    test_precision.update(preds, Y)\n",
        "    test_recall.update(preds, Y)\n",
        "    test_f1_score.update(preds, Y)\n",
        "  # Print the results\n",
        "  print(\"Confusion Matrix:\\n\", test_confusion_matrix.compute())\n",
        "  print(\"Accuracy:\", test_acc.compute().item())\n",
        "  print(\"Precision:\", test_precision.compute().item())\n",
        "  print(\"Recall:\", test_recall.compute().item())\n",
        "  print(\"F1 Score:\", test_f1_score.compute().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpwugH49G5tB"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap for better confusion matrix visualization\n",
        "sns.heatmap(test_confusion_matrix.compute().cpu(), annot=True, fmt=\"d\",\n",
        "            cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "# Labels and title\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOvSfUfWvHoP"
      },
      "source": [
        "**simple Grad-CAM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RICmbUiLSYlH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Function to generate Grad-CAM\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Hook to capture gradients\n",
        "        target_layer.register_forward_hook(self.save_activation)\n",
        "        target_layer.register_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def generate(self, input_image, target_class=None):\n",
        "        self.model.eval()\n",
        "        output = self.model(input_image)\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1)\n",
        "\n",
        "        loss = output[0, target_class]\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        gradients = self.gradients[0].cpu().data.numpy()\n",
        "        activations = self.activations[0].cpu().data.numpy()\n",
        "\n",
        "        weights = np.mean(gradients, axis=(1, 2))\n",
        "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
        "\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[i]\n",
        "\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (224, 224))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "# Pick a sample from test set\n",
        "sample_img, sample_label = next(iter(test_dataloader))\n",
        "sample_img = sample_img[0].unsqueeze(0).to(device)  # take one image\n",
        "\n",
        "# Apply Grad-CAM\n",
        "target_layer = model.layer4[-1]  # usually last layer for ResNet18\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "cam = gradcam.generate(sample_img)\n",
        "\n",
        "# Plot original + heatmap\n",
        "img = sample_img.cpu().squeeze().permute(1,2,0).numpy()\n",
        "img = (img - img.min()) / (img.max() - img.min())  # normalize image\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Grad-CAM Heatmap\")\n",
        "plt.imshow(img)\n",
        "plt.imshow(cam, cmap='jet', alpha=0.5)  # overlay Grad-CAM\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9hZMxkrwad4"
      },
      "source": [
        "# Saving The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zleEEkl9Qrw8"
      },
      "outputs": [],
      "source": [
        "# To save the best model\n",
        "best_val_loss = float('inf')  # Initialize best_val_loss to infinity\n",
        "\n",
        "if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), \"best_model6.pth\")\n",
        "    print(\"Best model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05caf01b"
      },
      "source": [
        "## ðŸ”§ Step 1: Hyperparameter Tuning\n",
        "\n",
        "We'll explore different hyperparameter settings to improve model performance:\n",
        "- Optimizer: Try switching from Adam to AdamW\n",
        "- Learning Rate: Test different values (1e-3, 1e-4, 5e-5)\n",
        "- Scheduler: Add ReduceLROnPlateau to adjust learning rate dynamically\n",
        "- Epochs: Increase from default (if low)\n",
        "- Batch Size: Experiment with smaller/larger values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8addca07"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and learning rate scheduler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "learning_rate = 1e-4  # Try: 1e-3, 5e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "\n",
        "# Scheduler that reduces LR when a metric has stopped improving\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b90215e"
      },
      "outputs": [],
      "source": [
        "# Modified training loop to include scheduler step\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss, train_acc = train_one_epoch(model, train_dataloader)\n",
        "    val_loss, val_acc = validation_one_epoch(model, test_dataloader)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}